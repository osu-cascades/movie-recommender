{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 406 Movie Recommender\n",
    "\n",
    "### Ryder McDowell\n",
    "\n",
    "#### OSU Cascades\n",
    "\n",
    "This matrix factorization algorithm uses AWS’s built in factorization machines and builds a model that allows clients to hit the deployed endpoint and receive predictions for new movies that a user may rate highly based on their previously rated movies and the similarity of that matrix to others’. This type of algorithm works best with extremely sparse data, so the features are one-hot encoded in order to make the vectors sparse to a degree of 2 (userID and movieID) of number of users plus number of movies.\n",
    "\n",
    "In order to receive predictions from a client, it can make a request to this lambda endpoint:\n",
    "\n",
    "https://github.com/osu-cascades/movie-recommender-lambda\n",
    "\n",
    "With the request body as json containing a list of userIDs and movieIDs to get predicted:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"samples\": [\n",
    "    {\n",
    "      \"userId\": 1,\n",
    "      \"movieId\": 20\n",
    "    },\n",
    "    {\n",
    "      \"userId\": 1,\n",
    "      \"movieId\": 33\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This will return a response that contains a matching list that provides whether or not the model thinks the user will rate that movie above a 3 stars and to what confidence level.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"predictions\": [\n",
    "        {\n",
    "            \"prediction_score\": 0.84,\n",
    "            \"predicted_label\": 1\n",
    "        },\n",
    "        {\n",
    "            \"prediction_score\": 0.62,\n",
    "            \"predicted_label\": 0\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Dataset used: https://grouplens.org/datasets/movielens/  \n",
    "Tutorial followed at: https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data:\")\n",
    "!head -10 ./ml-100k/ua.base\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "!head -10 ./ml-100k/ua.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n",
    "\n",
    "import boto3, csv, io, json\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(csv_reader):\n",
    "    samples = []\n",
    "    for userId,movieId,rating,timestamp in csv_reader:\n",
    "        samples.append({\n",
    "            'userId': userId,\n",
    "            'movieId': movieId,\n",
    "            'rating': rating,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        \n",
    "    return samples\n",
    "    \n",
    "    \n",
    "def get_maximums(samples):\n",
    "    users = []\n",
    "    movies = []\n",
    "    for sample in samples:\n",
    "        users.append(int(sample['userId']))\n",
    "        movies.append(int(sample['movieId']))\n",
    "\n",
    "    max_user_id = max(users)\n",
    "    max_movie_id = max(movies)\n",
    "    \n",
    "    return max_user_id, max_movie_id\n",
    "\n",
    "\n",
    "def get_matrix_shape(max_user_id, max_movie_id, samples):\n",
    "    total_samples = len(samples)\n",
    "    total_features = max_user_id + max_movie_id\n",
    "\n",
    "    return total_samples, total_features\n",
    "\n",
    "\n",
    "def fill_data(data, labels, samples):\n",
    "    row = 0\n",
    "        \n",
    "    # Build matrix and labels\n",
    "    for sample in samples:\n",
    "\n",
    "        # One hot-encode userId and movieId at row\n",
    "        user_index = int(sample['userId']) - 1\n",
    "        movie_index = 943 + int(sample['movieId']) - 1    #!!\n",
    "\n",
    "        data[row, user_index] = 1\n",
    "        data[row, movie_index] = 1\n",
    "\n",
    "        # Append binary to labels for whether user \"enjoyed\" movie\n",
    "        if int(sample['rating']) >= 4:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "        row = row + 1\n",
    "\n",
    "    # Convert labels list to float 32\n",
    "    labels = np.array(labels).astype('float32')\n",
    "    \n",
    "    return data, labels\n",
    "    \n",
    "\n",
    "def load_dataset(training_data_file_path, testing_data_file_path):\n",
    "    # Training Data\n",
    "    with open(training_data_file_path, 'r') as file:\n",
    "        csv_reader = csv.reader(file, delimiter='\\t')\n",
    "        \n",
    "        # Get all training samples in form of [{}, {}, ...]\n",
    "        training_samples = get_samples(csv_reader)\n",
    "        \n",
    "        # Get maximum number of users and movies\n",
    "        max_user_id, max_movie_id = get_maximums(training_samples)\n",
    "        \n",
    "        # Get shape of training matrix\n",
    "        training_matrix_shape = get_matrix_shape(max_user_id, max_movie_id, training_samples)\n",
    "        \n",
    "        # Initialize training data and labels structures\n",
    "        training_data = lil_matrix(training_matrix_shape).astype('float32')\n",
    "        training_labels = []\n",
    "\n",
    "        # Fill training data and labels structures with sample training data \n",
    "        training_data, training_labels = fill_data(training_data, training_labels, training_samples)\n",
    "        \n",
    "    # Testing Data\n",
    "    with open(testing_data_file_path, 'r') as file:\n",
    "        csv_reader = csv.reader(file, delimiter='\\t')\n",
    "        \n",
    "        # Get all testing samples in form of [{}, {}, ...]\n",
    "        testing_samples = get_samples(csv_reader)\n",
    "        \n",
    "        #Get shape of testing matrix\n",
    "        testing_matrix_shape = get_matrix_shape(max_user_id, max_movie_id, testing_samples)\n",
    "        \n",
    "        # Initialize testing data and labels structures\n",
    "        testing_data = lil_matrix(testing_matrix_shape).astype('float32')\n",
    "        testing_labels = []\n",
    "        \n",
    "        # Fill testing data and labels structurs with sample testing data\n",
    "        testing_data, testing_labels = fill_data(testing_data, testing_labels, testing_samples)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return (training_data, training_labels), (testing_data, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file_path = './ml-100k/ua.base'\n",
    "testing_data_file_path = './ml-100k/ua.test'\n",
    "\n",
    "(training_data, training_labels), (testing_data, testing_labels) = load_dataset(training_data_file_path, testing_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Ratings, Features)\")\n",
    "print(training_data.shape)\n",
    "print(training_labels.shape)\n",
    "\n",
    "print(testing_data.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[1000:1005])\n",
    "print(training_labels[1000:1005])\n",
    "\n",
    "print(training_data[1000:1005])\n",
    "print(testing_labels[1000:1005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:0.2f}% Movies Rated Above 3 in Training Data\".format(np.count_nonzero(training_labels) / training_data.shape[0] * 100))\n",
    "print(\"{:0.2f}% Movies Rated Above 3 in Testing Data\".format(np.count_nonzero(testing_labels) / testing_data.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparcity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_values = training_data.shape[0] * 2\n",
    "total_values = training_data.shape[0] * training_data.shape[1]\n",
    "\n",
    "print(\"{:0.5f}% Sparse\".format(100 - (encoded_values / total_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'rydermcdowell-sagemaker'\n",
    "prefix = 'fm-movielens'\n",
    "\n",
    "training_data_key = '{}/training-data/training.protobuf'.format(prefix)\n",
    "testing_data_key = '{}/testing-data/testing.protobuf'.format(prefix)\n",
    "\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset_to_protobuf(data, labels, bucket, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, data, labels)\n",
    "    buf.seek(0)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(key).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_location = write_dataset_to_protobuf(training_data, training_labels, bucket, training_data_key)\n",
    "testing_data_location = write_dataset_to_protobuf(testing_data, testing_labels, bucket, testing_data_key)\n",
    "\n",
    "print('Training data written to: {}'.format(training_data_location))\n",
    "print('Testing data written to: {}'.format(testing_data_location))\n",
    "print('Output location: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "                'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "                'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "                'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "                'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                   get_execution_role(),\n",
    "                                   train_instance_count = 1,\n",
    "                                   train_instance_type = 'ml.m5.large',\n",
    "                                   output_path = output_path,\n",
    "                                   sagemaker_session = sagemaker.Session()\n",
    "                                  )\n",
    "\n",
    "fm.set_hyperparameters(feature_dim = training_data.shape[1],\n",
    "                       predictor_type = 'binary_classifier',\n",
    "                       mini_batch_size = 1000,\n",
    "                       num_factors = 64,\n",
    "                       factors_lr = 0.01,\n",
    "                       epochs = 200\n",
    "                      )\n",
    "\n",
    "fm.fit({ 'train': training_data_location, 'test': testing_data_location })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type = 'ml.m5.large', initial_instance_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(data):\n",
    "    js = { 'instances': [] }\n",
    "    for row in data:\n",
    "        js['instances'].append({ 'features': row.tolist() })\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fm_predictor.predict(testing_data[4].toarray())\n",
    "print(\"{}\\n\".format(result))\n",
    "print(\"{}\".format(map(lambda prediction: prediction['predicted_label'], result['predictions'])))\n",
    "print(testing_labels[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
